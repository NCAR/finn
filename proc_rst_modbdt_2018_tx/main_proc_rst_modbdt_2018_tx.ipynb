{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# FINN Preprocessor: Process MODIS Burned Area into polygon Shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Envoronments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Systems settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most likely no need to be edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import subprocess\n",
    "import shlex\n",
    "from urllib.parse import urlparse\n",
    "from importlib import reload\n",
    "import gdal\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "# finn preproc codes\n",
    "sys.path = sys.path + ['../code_anaconda']\n",
    "import downloader\n",
    "import af_import\n",
    "import rst_import\n",
    "import polygon_import\n",
    "import run_step1\n",
    "import run_step2\n",
    "import export_shp\n",
    "import plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database settings\n",
    "os.environ['PGDATABASE'] = 'gis'\n",
    "os.environ['PGUSER'] = 'finn'\n",
    "os.environ['PGPASSWORD'] = 'finn'\n",
    "os.environ['PGHOST'] = 'localhost'\n",
    "os.environ['PGPORT'] = '5432'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the PostGIS database is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show info for the database\n",
    "!psql postgres -c 'SELECT version();'\n",
    "!pg_lsclusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO i want to move this to Dockerfile somehow\n",
    "# create plpython, needed only once for the database\n",
    "try:\n",
    "    p = subprocess.run(shlex.split(\"psql -d %s -c 'CREATE LANGUAGE plpython3u;'\" % os.environ['PGDATABASE']), \n",
    "                       check=True, stderr=subprocess.PIPE)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    if 'already exists' in e.stderr.decode():\n",
    "        print(e.stderr.decode().replace('ERROR','OK').strip())\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings for Burnt Area Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIS burned area will be downloaded, if needed, for the region specified by the rectangle\n",
    "1. `year_rst`, `month_rst`: MODIS burned area year/month to be processed\n",
    "2. either  \n",
    "  `four_corners`: (LowerLeft_lon, LowerLeft_Lat, UpperRight_lon, UpperRight_lat) or   \n",
    "  `extent_shp`:  shape file (could be polygon of area of interest, points of fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS raster datasets' year\n",
    "#first_year_rst = 2017\n",
    "#first_month_rst = 1\n",
    "first_year_rst = 2018\n",
    "first_month_rst = 1\n",
    "last_year_rst = 2018\n",
    "last_month_rst = 11\n",
    "\n",
    "# tag to identify dataset\n",
    "tag_bdt = 'modbdt_2018_tx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic extent of download\n",
    "# specify either one of below (comment out one line with #)\n",
    "\n",
    "four_corners = (-107, 25, -93, 37) # LL corner lon, LL corner LAT, UR corner Lon, UR corner Lat)\n",
    "#extent_shp = './north_central_america.shp'  # shape file of North and Central America (i can create this from AllRegion polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year/month series\n",
    "yrmo0 = datetime.date(first_year_rst, first_month_rst, 1)\n",
    "yrmo1 = datetime.date(last_year_rst, last_month_rst, 1)\n",
    "yrmos = [yrmo0 + datetime.timedelta(days=_) for _ in range((yrmo1-yrmo0).days)]\n",
    "yrmos = [_ for _ in yrmos if _.day == 1]\n",
    "yrmos.append(yrmo1)\n",
    "\n",
    "\n",
    "# tags to identify datasets, automatically set to be modlct_YYYY, modvcf_YYYY\n",
    "#tag_bdt = 'modbdt_%d%02d' % (year_rst, month_rst)\n",
    "tags_bdt = ['modbdt_%d%02d' % (yrmo.year, yrmo.month) for yrmo in yrmos]\n",
    "tags_bdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Download raster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raster files URL and directories to save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all downloads are stored in following dir\n",
    "download_rootdir = '../downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthdata's URL for BA\n",
    "#url_bdt = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD64A1.006/%d.%02d.01/' % (year_rst, month_rst)\n",
    "urls_bdt = ['https://e4ftl01.cr.usgs.gov/MOTA/MCD64A1.006/%d.%02d.01/' % (yrmo.year, yrmo.month) for yrmo in yrmos]\n",
    "\n",
    "#ddir_bdt = download_rootdir +'/'+ ''.join(urlparse(url_bdt)[1:3])\n",
    "ddirs_bdt = [download_rootdir +'/'+ ''.join(urlparse(url_bdt)[1:3]) for url_bdt in urls_bdt]\n",
    "\n",
    "print('BDT downloads goes to %s' % ddirs_bdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download burned area raster, <b>only for the tiles needed for the active fire file</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'four_corners' in locals() and not four_corners is None:\n",
    "    # use four corner\n",
    "    poly = \"POLYGON((%f %f, %f %f, %f %f, %f %f, %f %f))\" % (\n",
    "        four_corners[0], four_corners[1], \n",
    "        four_corners[0], four_corners[3], \n",
    "        four_corners[2], four_corners[3], \n",
    "        four_corners[2], four_corners[1],\n",
    "        four_corners[0], four_corners[1], \n",
    "    )\n",
    "    knd = 'wkt'\n",
    "elif 'extent_shp' in locals() and not extent_shp is None:\n",
    "    # use shape file\n",
    "    poly = extent_shp\n",
    "    knd = 'ds'\n",
    "else:\n",
    "    raise RuntimeError('Specify region of interest!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(downloader)\n",
    "for url_bdt in urls_bdt:\n",
    "    downloader.download_only_needed(url = url_bdt, droot = download_rootdir, region=poly, region_knd=knd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify BDT files' checksum.  If a file is corrupted, the file is downloaded again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ddir_bdt, url_bdt in zip(ddirs_bdt, urls_bdt):\n",
    "    downloader.purge_corrupted(ddir = ddir_bdt, url=url_bdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import raster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded files need preprocessing, which is to extract the only raster band needed, and also make coordinate system to be WGS84.  Intermediate files are created in following directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir_bdt = '../proc_rst_%s' % tag_bdt\n",
    "\n",
    "print('BDT preprocessing occurs in %s' % workdir_bdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Buned Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First grab hdf file names from the download directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_bdt = {}\n",
    "n = 0\n",
    "for ddir_bdt, yrmo in zip(ddirs_bdt, yrmos):\n",
    "    search_string = \"%(ddir_bdt)s/MCD64A1.A%(year_rst)s???.h??v??.006.*.hdf\" % dict(\n",
    "            ddir_bdt = ddir_bdt, year_rst=yrmo.year)\n",
    "    fnames_bdt[yrmo] = sorted(glob.glob(search_string))\n",
    "    n += len(fnames_bdt[yrmo])\n",
    "print('found %d hdf files' % n )\n",
    "if n == 0:\n",
    "    raise RuntimeError(\"check if downloads are successful and search string to be correct: %s\" % search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command performs three tasks, \"merge\", \"resample\" and \"import\".  First two task creates intermediate GeoTiff files in <i>work_dir</i>.  Last task actually import the data into database's <i>raster</i> schema.\n",
    "\n",
    "You can run only selected tasks with run_XXX flags to `False`, when you know that processing failed in the middle and you resolved the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rst_import)\n",
    "for yrmo in yrmos:\n",
    "    print(yrmo)\n",
    "    rst_import.main(tag_bdt, fnames=fnames_bdt[yrmo], workdir = workdir_bdt, \n",
    "                    run_merge=True, run_resample=True, run_import=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge shapes into one\n",
    "import ogr\n",
    "for knd in ('poly', 'pnt'):\n",
    "    shpfiles = glob.glob(os.path.join(workdir_bdt, 'rsp', 'MCD64A1.A???????.%s.shp' % knd))\n",
    "    oname = os.path.join(workdir_bdt, '.'.join(('MCD64A1', tag_bdt, knd, 'shp' )))\n",
    "    if os.path.exists(oname):\n",
    "        drv = ogr.GetDriverByName('ESRI Shapefile')\n",
    "        drv.DeleteDataSource(oname)\n",
    "    cmd = ['ogr2ogr', '-update', '-append', oname, ]\n",
    "    for shp in shpfiles:\n",
    "        cmdx = cmd + [shp]\n",
    "        subprocess.run(cmdx, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
