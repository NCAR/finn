{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINN Preprocessor\n",
    "\n",
    "This notebook contains the code to preprocess active fire data for the FINN fire emissions model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. User specified configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the FINN preprocessor, you should specify the following: \n",
    "\n",
    "1. `tag_af`: a tag or name for your active fire dataset, e.g., 'modvrs_global_2018'.  See below for guidance for picking `tag_af`.\n",
    "2. `af_fnames`: a list of file paths to active fire shape files (e.g., downloaded from [FIRMS](https://firms.modaps.eosdis.nasa.gov/)) \n",
    "3. `year_rst`: MODIS raster data year to be used for the analysis.  It is recommended to use the data from a year prior to the Active Fire data, as the impact of large fire may affects the VCF product of the year, resulting in smaller estimate of fuel loadings.\n",
    "4. `filter_persistent_source`: ActiveFire product has inferred hot spot \"type\" field (0 = presumed vegetation fire, 1 = active volcano, 2 = other static land source, 3 = offshore) ref: Table 9 of [Giglio et al. \"MODIS Collection 6 Active Fire Product Userâ€™s Guide Revision B\" 2018](https://cdn.earthdata.nasa.gov/conduit/upload/10575/MODIS_C6_Fire_User_Guide_B.pdf).  By making this option True, \"1 active volcano\" and \"2 other static land source\" are dropped.  Recommended to keep this True\n",
    "5. `export_frp`: experimental implementation to export arithtmetic average frp value of all accepted observations within each divided polygon.  Default is False, not to calculate this.\n",
    "6. `download_global_raster`:  By making this option to True, code downloads all HDF raster LCT/VCF file for specified year.  By making this false, it identify necessary HDF files enough to cover the extent of AF input.  For regional/continental application with fixed area of interest, this would make sense.  For global application, or multiple continent application, or your area of interest changes a lot across runs, it may make sense to make this True to grab everything, because finding which HDF file is needed does take some time.  Default value is False.\n",
    "\n",
    "It is recommended to use descriptive name which identified data source (modis vs viirs), spatial extent and time period.  e.g. `modvrs_global_2018` for Modis/VIIRS combined detection for global scale modeling for year 2018, or `mod_tx_2012sum` for modis detection for Texas for Summer 2012 (definition of \"summer\" is implicit here).\n",
    "\n",
    "The variable `tag_af` is used as part of PostGIS database schema and table (\"[SQL Identifier](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS)\"), and you have to start with letter (a-z, A-Z) or underscore (\\_) followed by letter/underscore/digit (0-9).\n",
    "\n",
    "Final output files will be named \"out_<i>tag_af</i>_*.csv\" and \"out_<i>tag_af</i>_*.shp\".\n",
    "\n",
    "It is also recommended that you create copy of directory contining this notebook (`work_generic`) and process your custom run there, so that one activefire procerssing is housed in different directory.  Main reason is that this notebook is tied to GitHub repository.  So if the repository got updated and if you want to pull the changes, it may causes conflict if this file had modifications by you.  By making copy before you edit, main_generic.ipynb can be updated from repository.  \n",
    "\n",
    "### Default settings\n",
    "\n",
    "To use a small example dataset that is bundled with the FINN preprocessor, you can use the following settings:\n",
    "\n",
    "```python\n",
    "tag_af = 'testOTS_092018'\n",
    "\n",
    "af_fnames = [\n",
    "    '../sample_datasets/fire/testOTS_092018/fire_archive_M6_23960.shp',\n",
    "    '../sample_datasets/fire/testOTS_092018/fire_archive_V1_23961.shp',\n",
    "]\n",
    "\n",
    "year_rst = 2017\n",
    "\n",
    "filter_persistent_sources = True\n",
    "\n",
    "download_global_raster = False\n",
    "```\n",
    "\n",
    "### Custom settings with MODIS and VIIRS\n",
    "\n",
    "**If you have both custom MODIS and VIIRS shapefiles** you must specify the file path(s) to the active fire data shapefiles. Ensure that the shapefiles are unzipped and placed within the `finn_preproc/data/` directory.  You may specify as many files of mixed type (MODIS or VIIRS, archived data vs near real time data).  Tool is go through each dataset, identify the datasource, and process accordingly.  All data are used simultaneously to yield one unified set of outputs.\n",
    "\n",
    "It is encouraged to use `tag_af` variable wisely, as explained earlier.  For example, \"modvrs_conus_2016\" to represents MODIS/VIIRS hybrid processing, for contiguous united state region, for year 2016.  In other words, `{data_source}_{geographic_region}_{time_period}` would be a possible convention of yours.\n",
    "\n",
    "It is also recommended to use raster from a year before the fire, as explained above.\n",
    "\n",
    "```python\n",
    "tag_af = 'modvrs_global_2018'\n",
    "\n",
    "af_fname = [\n",
    "    '../data/fire_archive_M6_...',\n",
    "    '../data/fire_archive_V1_...'\n",
    "]\n",
    "\n",
    "year_rst = 2017\n",
    "\n",
    "filter_persistent_sources = True\n",
    "\n",
    "download_global_raster = True\n",
    "```\n",
    "\n",
    "### Custom settings with MODIS or VIIRS\n",
    "\n",
    "Alternatively, if you have just MODIS or VIIRS, you can specify one file path.  For example, in the example below there is only one M6 file provided (supposedly for summer 2012 for Texas).  \n",
    "\n",
    "```python\n",
    "tag_af = 'mod_tx_2012sum'\n",
    "\n",
    "af_fname = [\n",
    "    '../data/fire_archive_M6_...',\n",
    "]\n",
    "\n",
    "year_rst = 2011\n",
    "\n",
    "filter_persistent_sources = True\n",
    "\n",
    "download_global_raster = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_af = 'testOTS_092018'\n",
    "\n",
    "af_fnames = [\n",
    "    '../sample_datasets/fire/testOTS_092018/fire_archive_M6_23960.shp',\n",
    "    '../sample_datasets/fire/testOTS_092018/fire_archive_V1_23961.shp',\n",
    "]\n",
    "\n",
    "year_rst = 2017\n",
    "\n",
    "filter_persistent_sources = True\n",
    "\n",
    "export_frp = True\n",
    "\n",
    "download_global_raster = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as default set of raster (MODIS LCT, MODIS VCF and global region definition of Wiedinmyer 2011), no need to edit following sections.  However below specifies data source of raster (in terms of table name inside `raster` schema) and how data is processed.  `variable` specifies variable name in the final output CSV file.  `kind` field specifies if the data are to be treated as thematic (categorical) data or continous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag to identify datasets, automatically set to be modlct_YYYY, modvcf_YYYY\n",
    "tag_lct = 'modlct_%d' % year_rst\n",
    "tag_vcf = 'modvcf_%d' % year_rst\n",
    "\n",
    "# tag for the region number polygon\n",
    "tag_regnum = 'regnum'\n",
    "\n",
    "# definition of variables in the raster files\n",
    "rasters = [\n",
    "        {\n",
    "            'tag': tag_lct,\n",
    "            'kind': 'thematic',\n",
    "            'variable': 'lct'\n",
    "        },\n",
    "        {\n",
    "            'tag': tag_vcf,\n",
    "            'kind': 'continuous',\n",
    "            'variables': ['tree', 'herb', 'bare'],\n",
    "        },\n",
    "        {\n",
    "            'tag': tag_regnum,\n",
    "            'kind': 'polygons',\n",
    "            'variable_in': 'region_num',\n",
    "            'variable': 'regnum',\n",
    "        },\n",
    "]\n",
    "\n",
    "if export_frp:\n",
    "    tag_frp = 'frp'\n",
    "    rasters.append(\n",
    "        {\n",
    "            'tag': tag_frp,\n",
    "            'kind': 'input',\n",
    "            'variable_in': 'frp',\n",
    "            'variable': 'frp',\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options below specifies if extra output/log will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save *.shp of the output, so that you can hold onto polygons\n",
    "save_shp_of_output = False\n",
    "\n",
    "# save *.html version of this notebook upon exit, so that you can keep records\n",
    "save_html_of_notebook = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options below will delete the intermediate files at section 8, after the processing is finished.  Make these into True if you know that this notebook work (with your settings) and you don't need to review the intermediates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes entire schema in the database for the AF data processed in this notebook\n",
    "wipe_intermediate_vector_in_db = False\n",
    "\n",
    "# like above, but not deleting entire schema, but delete only giant intermediate table in schema, \n",
    "# so that you can still QA the data in db,if needed \n",
    "# (make wipe_intermediate_vector_in_db to be False to use this feature)\n",
    "clean_intermediate_vector_in_db = True\n",
    "\n",
    "# deletes hdf files downloaded from EARTHDATA for particular year used in this notebook\n",
    "wipe_downloaded_hdf = True\n",
    "\n",
    "# deletes intermediate geotiff files (found in proc_rst_XXX directory) for particular year used in this notebook\n",
    "wipe_intermediate_geotiff = True\n",
    "\n",
    "# deletes table of raster data imported into database (praticular year used in this notebook)\n",
    "wipe_intermediate_rst_in_db = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of code below should run without modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generic preparation of the preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "system (linux) environment for debugging purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!env | sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For QA purpose, echo what version of postgresql/postgis got used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!psql -d finn -c \"select version();\"\n",
    "!psql -d finn -c \"select postgis_full_version();\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python version used in postgresql/plpython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!psql -d finn -f ../code_anaconda/testpy.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import subprocess\n",
    "import time\n",
    "import shlex\n",
    "from urllib.parse import urlparse\n",
    "from importlib import reload\n",
    "try:\n",
    "    import gdal\n",
    "except ImportError:\n",
    "    from osgeo import gdal\n",
    "import matplotlib.pylab as plt\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# finn preproc codes\n",
    "sys.path = sys.path + ['../code_anaconda']\n",
    "import downloader\n",
    "import af_import\n",
    "import rst_import\n",
    "import polygon_import\n",
    "import run_step1a\n",
    "import run_step1b\n",
    "import run_vcf\n",
    "import run_step2\n",
    "import export_shp\n",
    "import plotter\n",
    "import run_extra\n",
    "import notebook_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need PL/Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    subprocess.run(['psql', '-d', 'finn', '-c', 'CREATE LANGUAGE plpython3u;'], check=True, stderr=subprocess.PIPE)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    if 'already exists' in e.stderr.decode():\n",
    "        print(e.stderr.decode().replace('ERROR','OK').strip())\n",
    "    else:\n",
    "        print('\\n\\nGot Error!!!')\n",
    "        print(e.stderr.decode())\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need wireframe of modis files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(rst_import)\n",
    "rst_import.prep_modis_tile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import AF dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test active fire data files exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular sample AF dataset are provided by FINN developper.  In other applications, it will be user's resoponsibility to provide shape file for active fire in specified path/name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check input file exists\n",
    "print('checking if input files exist:')\n",
    "re_shp = re.compile('fire_archive_(.*).shp')\n",
    "re_zip = re.compile('DL_FIRE_(.*).shp')\n",
    "re_shp_nrt = re.compile('(MODIS_C6|VNP14IMGTDL_NRT)_(.*).shp')\n",
    "\n",
    "\n",
    "for i,af_fname in enumerate(af_fnames):\n",
    "    print(\"%s: \" % af_fname, end='')\n",
    "    \n",
    "    pn,fn = os.path.split(af_fname)\n",
    "    zname = None\n",
    "    \n",
    "    if os.path.exists(af_fname):\n",
    "        print(\"exists.\")\n",
    "        # if .zip file, need to expand.\n",
    "        if af_fname[-4:] in ('.shp', '.csv'):\n",
    "            # you are good\n",
    "            print('OK')\n",
    "        \n",
    "        elif af_fname[-4:] == '.zip':\n",
    "            # still need to unzip\n",
    "            zname = af_fname\n",
    "            # asssume that this ls DL_FIRE*.zip file from FIRMS\n",
    "            # look for shape file name\n",
    "            import zipfile\n",
    "            myshp = [_ for _ in zipfile.ZipFile(zname).namelist() if _.endswith('.shp')]\n",
    "            if len(myshp) != 0:\n",
    "                sname = myshp[0]\n",
    "                m = re_shp.match(sname)\n",
    "                assert m\n",
    "            else:\n",
    "                raise RuntileError('specify .shp file in af_names list!')\n",
    "                arcname,sname = None, None\n",
    "                    \n",
    "            \n",
    "        else:\n",
    "            raise RuntimeError('specify .shp or .csv file in af_names list!')\n",
    "    else:\n",
    "        print(\"doesn't exist.\")\n",
    "        \n",
    "        if af_fname[-4:] == '.shp':\n",
    "            # guess the zip file name\n",
    "            \n",
    "            pn,fn=os.path.split(af_fname)\n",
    "            \n",
    "            # see if it's the sample giant archive we provide \n",
    "            if fn == 'fire_archive_M6_28864.shp':\n",
    "                zurl = 'https://s3-us-west-2.amazonaws.com/earthlab-finn/2016-global-DL_FIRE_M6_28864.zip'\n",
    "                zn = '2016-global-DL_FIRE_M6_28864.zip'\n",
    "                zname = os.path.join(pn, zn)\n",
    "                sname = fn\n",
    "                if not os.path.exists(zname):\n",
    "                    print('downloading the sample AF file: %s' % zn)\n",
    "                    subprocess.run(['wget', '-P', pn, zurl], check=True)\n",
    "            else:\n",
    "\n",
    "                # see if it's an archive of AF\n",
    "                m = re_shp.match(fn)\n",
    "                if m:\n",
    "                    arcname = m.groups()[0]\n",
    "                    zname = os.path.join( pn, 'DL_FIRE_%s.zip' % arcname)\n",
    "                    sname = fn\n",
    "                    print('  found zip: %s' % zname)\n",
    "                else:\n",
    "                    # see if it's NRT data\n",
    "                    m = re_shp_nrt.match(fn)\n",
    "\n",
    "                    if m:\n",
    "                        # NRT downloads\n",
    "                        zname = af_fname[:-4] + '.zip'\n",
    "                        sname = fn\n",
    "                        print('  found zip: %s' % zname)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        raise RuntimeError('cannot find file: %s' % af_fname)\n",
    "        else:\n",
    "            raise RuntimeError('cannot find file: %s' % af_fname)\n",
    "    if zname:\n",
    "        print('unzipping: %s' % zname)\n",
    "        subprocess.run(['unzip', '-uo', zname, '-d', os.path.dirname(zname)],\n",
    "                      check=True)\n",
    "        assert os.path.exists(os.path.join(pn, sname))\n",
    "        af_fnames[i] = os.path.join(pn, sname)\n",
    "        print('OK: done')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import active fire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and import into database.\n",
    "\n",
    "<b>Be careful!!</b> The code has no safe guard and wipe the schema for the scheama \"af_<i>tag_af</i>\" and starts over.  \n",
    "\n",
    "Let me think the design a bit more for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(af_import)\n",
    "\n",
    "# TODO this is destructive need to safe guard!\n",
    "# tell user schema is there, list table names and # of row of each.  Ask her to delete manually or something to proceed\n",
    "af_import.main(tag_af, af_fnames)\n",
    "\n",
    "print()\n",
    "for i,fn in enumerate(af_fnames):\n",
    "    print(fn)\n",
    "    tblname = '\"af_%s\".af_in_%d' % (tag_af, i+1)\n",
    "    p = subprocess.run(['psql', '-c', 'select count(*) from %s;' % tblname], stdout=subprocess.PIPE)\n",
    "    print(p.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show spatial distribution of imported active fire, just for QA purpose.  You should also be able to use QGIS to visualize actual imported data as schema name \"af_XXX\", table name \"af_in_N\", where XXX is <i>tag_af</i> you specified in the first box, N is serial number for active fire files you listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(plotter)\n",
    "\n",
    "try:\n",
    "    plotter.plotter.plot(['\"af_%s\".af_in_%d' % (tag_af, i+1)\n",
    "                     for i,fn in enumerate(af_fnames)], density=True)\n",
    "except Exception as e:\n",
    "    print(\"Got this error: \" + str(e))\n",
    "    print(\"Didn't work, use QGIS!\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download raster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the extent of raster dataset in the database encloses all fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(downloader)\n",
    "if download_global_raster:\n",
    "    results_indb = downloader.find_tiles_indb(data='POLYGON((-180 89,-180 -89,180 -89,180 89,-180 89))', \n",
    "                                             knd='wkt', tag_lct=tag_lct, tag_vcf=tag_vcf)\n",
    "else:\n",
    "    results_indb = downloader.find_tiles_indb(data='\"af_%s\"' % tag_af, \n",
    "                                              knd='schema', tag_lct=tag_lct, tag_vcf=tag_vcf)\n",
    "print(results_indb)\n",
    "print()\n",
    "\n",
    "\n",
    "if results_indb['n_need'] == 0:\n",
    "    print('All fire are is conained in raster')\n",
    "    print('no need to download/import raster dataset')\n",
    "    need_to_import_lct = False\n",
    "    need_to_import_vcf = False\n",
    "else:\n",
    "    print('Some fire are not conained in raster')\n",
    "    print('Will download/import raster dataset')\n",
    "    need_to_import_lct = (len(results_indb['tiles_missing_lct']) > 0)\n",
    "    need_to_import_vcf = (len(results_indb['tiles_missing_vcf']) > 0)\n",
    "    tiles_required_lct = results_indb['tiles_required_lct']\n",
    "    tiles_required_vcf = results_indb['tiles_required_vcf']\n",
    "\n",
    "print()\n",
    "reload(downloader)\n",
    "need_to_import_regnum = not downloader.find_table_indb('raster', 'rst_%s' % tag_regnum)\n",
    "if need_to_import_regnum:\n",
    "    print('Region definiton shapefile will be imported')\n",
    "else:\n",
    "    print('no need to import Region definiton shapefile')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range of active fire\n",
    "# TODO use this to set \"year_rst\" ?\n",
    "reload(af_import)\n",
    "af_dates = af_import.get_dates(schema = '\"af_%s\"'%tag_af, combined=True)\n",
    "print('first day in AF file:', af_dates.min())\n",
    "print('last day in AF file:', af_dates.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raster files URL and directories to save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all raster downloads are stored in following dir\n",
    "download_rootdir = '../downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthdata's URL for landcover and VCF\n",
    "is_leap = (year_rst % 4 == 0)\n",
    "url_lct = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.006/%d.01.01/' % year_rst\n",
    "url_vcf = 'https://e4ftl01.cr.usgs.gov/MOLT/MOD44B.006/%d.03.%02d/' % (year_rst, 5 if is_leap else 6)\n",
    "\n",
    "ddir_lct = download_rootdir +'/'+ ''.join(urlparse(url_lct)[1:3])\n",
    "ddir_vcf = download_rootdir +'/'+ ''.join(urlparse(url_vcf)[1:3])\n",
    "\n",
    "if any((need_to_import_lct, need_to_import_vcf)):\n",
    "    print('LCT downloads goes to %s' % ddir_lct)\n",
    "    print('VCF downloads goes to %s' % ddir_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download land cover type raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_lct:\n",
    "    reload(downloader)\n",
    "    downloader.download_only_needed(url = url_lct, droot = download_rootdir, tiles=tiles_required_lct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify LCT files' checksum.  If a file is correpted, the file is downloaded again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_lct:\n",
    "    downloader.purge_corrupted(ddir = ddir_lct, url=url_lct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do similar for vegetation continuous field data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if need_to_import_vcf:\n",
    "    downloader.download_only_needed(url = url_vcf, droot = download_rootdir, tiles=tiles_required_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_vcf:\n",
    "    downloader.purge_corrupted(ddir_vcf, url=url_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import raster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded files need preprocessing, which is to extract the only raster band needed, and also make coordinate system to be WGS84.  Intermediate files are created in following directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir_lct = '../proc_rst_%s' % tag_lct\n",
    "workdir_vcf = '../proc_rst_%s' % tag_vcf\n",
    "workdir_regnum = '../proc_rst_%s' % tag_regnum\n",
    "\n",
    "if need_to_import_lct:\n",
    "    print('LCT preprocessing occurs in %s' % workdir_lct)\n",
    "if need_to_import_vcf:\n",
    "    print('VCF preprocessing occurs in %s' % workdir_vcf)\n",
    "if need_to_import_regnum:\n",
    "    print('RegNum preprocessing occurs in %s' % workdir_regnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import land cover type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First grab hdf file names from the download directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_lct:\n",
    "    search_string = \"%(ddir_lct)s/MCD12Q1.A%(year_rst)s001.h??v??.006.*.hdf\" % dict(\n",
    "        ddir_lct = ddir_lct, year_rst=year_rst)\n",
    "    fnames_lct = sorted(glob.glob(search_string))\n",
    "    print('found %d hdf files' % len(fnames_lct) )\n",
    "    if len(fnames_lct) == 0:\n",
    "        raise RuntimeError(\"check if downloads are successful and search string to be correct: %s\" % search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command performs three tasks, \"merge\", \"resample\" and \"import\".  First two task creates intermediate GeoTiff files in <i>work_dir</i>.  Last task actually import the data into database's <i>raster</i> schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_lct:\n",
    "    reload(rst_import)\n",
    "    rst_import.main(tag_lct, fnames=fnames_lct, workdir = workdir_lct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should able to see the raster in the database using QGIS. \n",
    "It is located in schema \"raster\", table name either one of three, \"rst_modlct_YYYY\", \"o_32_rst_modlct_YYYY\", or \"o_256_rst_modlct_YYYY\", where YYYY is year of raster. \"o_\" version of raster were for visualization purpose small scale map (greater spatial extent) where as the table without \"o_\" is the actual data used in processing.\n",
    "\n",
    "I am also trying to make quick check here creating simple image for QA, but use of GIS tool is encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import plotter\n",
    "reload(plotter)\n",
    "try:\n",
    "    plotter.plotter.plot('raster.o_32_rst_%s' % tag_lct, '../code_anaconda/modlct.clr')\n",
    "except Exception as e:\n",
    "    try: \n",
    "        plotter.plotter.plot('raster.o_256_rst_%s' % tag_lct, '../code_anaconda/modlct.clr')\n",
    "    \n",
    "    except Exception as e2:\n",
    "        print(\"Got this error: \" + str(e))\n",
    "        print(\"Didn't work, use QGIS!\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import vegetation continuous fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous steps repeated for vegetation continous fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_vcf:\n",
    "    # grab hdf file names\n",
    "    search_string = \"%(ddir_vcf)s/MOD44B.A%(year)s065.h??v??.006.*.hdf\" % dict(\n",
    "            ddir_vcf = ddir_vcf, year=year_rst)\n",
    "    fnames_vcf = sorted(glob.glob(search_string))\n",
    "    print('found %d hdf files' % len(fnames_vcf) )\n",
    "    if len(fnames_vcf) == 0:\n",
    "        raise RuntimeError(\"check if downloads are successfull and search string to be correct: %s\" % search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_vcf:\n",
    "    reload(rst_import)\n",
    "    rst_import.main(tag_vcf, fnames=fnames_vcf, workdir = workdir_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import plotter\n",
    "reload(plotter)\n",
    "try:\n",
    "    plotter.plotter.plot('raster.o_32_rst_%s' % tag_vcf)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        plotter.plotter.plot('raster.o_256_rst_%s' % tag_vcf)\n",
    "    except Exception as e2:\n",
    "        print(\"Got this error: \" + str(e))\n",
    "        print(\"Didn't work, use QGIS!\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import countries of the world shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually not a raster but vector data of polygons.  But since it serves conceptually similar function as raster (specify attribute for a given geographic location), I treat it as if it is a raster dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_regnum:\n",
    "    if not os.path.exists(os.path.join(workdir_regnum, 'All_Countries.shp')):\n",
    "        subprocess.run(['wget', '-P', workdir_regnum,\n",
    "                        'https://s3-us-west-2.amazonaws.com/earthlab-finn/All_Countries.zip'], \n",
    "                       check=True)\n",
    "        subprocess.run(['unzip', os.path.join(workdir_regnum, 'All_Countries.zip'), '-d' , workdir_regnum ], \n",
    "                       check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_regnum:\n",
    "    reload(polygon_import)\n",
    "    polygon_import.main(tag_regnum, shpname = os.path.join(workdir_regnum, 'All_Countries.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process active fire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running \"step 1\" grouping points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"extended polygons\" by joining when scan/pixel extent touches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(run_step1a)\n",
    "run_step1a.main(tag_af, filter_persistent_sources = filter_persistent_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine tree fraction in the polygons deterimined by step1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(run_vcf)\n",
    "print(rasters)\n",
    "run_vcf.main(tag_af, rasters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create \"conservative polygons\" for detection with smaller tree covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(run_step1b)\n",
    "run_step1b.main(tag_af, filter_persistent_sources = filter_persistent_sources)#, \n",
    "#                first_day=datetime.date(2018,8,10), last_day=datetime.date(2018,8,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running \"step 2\" intersection with raster datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(run_step2)\n",
    "run_step2.main(tag_af, rasters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default output directory is this diretory (where you have this Jupyter Notebook file), and output file has long name of having tag of each datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_rasters = '_'.join([rst['tag'] for rst in rasters])\n",
    "\n",
    "outdir = '.'\n",
    "shpname = 'out_{tag_af}_{tag_rasters}.shp'.format(tag_af=tag_af, tag_rasters=tag_rasters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name of schema/table/fields in the database, all defined in `rasters` variables near the top of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'af_' + tag_af\n",
    "tblname = 'out_{tag_rasters}'.format(tag_rasters=tag_rasters)\n",
    "\n",
    "fields_to_export = []\n",
    "for rst in rasters:\n",
    "    if rst['kind'] == 'thematic':\n",
    "        fields_to_export.extend([prefix+'_'+rst['variable'] for prefix in ('v', 'f')])\n",
    "    else:\n",
    "        if 'variables' in rst:\n",
    "            fields_to_export.extend(['v_'+v for v in rst['variables']])\n",
    "        else:\n",
    "            fields_to_export.append('v_'+rst['variable'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(export_shp)\n",
    "export_shp.main(outdir, schema, tblname, fields_to_export, shpname, \n",
    "                csvonly=(not save_shp_of_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(run_extra)\n",
    "run_extra.summarize_log(tag_af)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Disk use summary and clean up (if you wish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see summary of disk use and code which optionally cleans up disk use by removing intermediates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point/Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All intermediate datasets for vector processing are stored in the database.  See the usage of individual tables and grand total below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_af = \"\"\"SELECT table_schema || '.' || table_name AS table_full_name,\n",
    "pg_size_pretty(pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"')) AS size\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = '%(sch_af)s'   \n",
    "ORDER BY pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"') DESC;\"\"\" % dict(\n",
    "    sch_af=('af_%s' % tag_af),\n",
    ")\n",
    "\n",
    "qry_af_tot = \"\"\"SELECT table_schema,\n",
    "pg_size_pretty(sum(pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"'))) AS size\n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = '%(sch_af)s'   \n",
    "GROUP BY table_schema;\"\"\" % dict(\n",
    "    sch_af=('af_%s' % tag_af),\n",
    ")\n",
    "\n",
    "\n",
    "print('Disk use by AF processing intermediate tables inside the database\\n')\n",
    "p = subprocess.run(['psql', '-d', 'finn', '-c', qry_af], stdout=subprocess.PIPE, check=True)\n",
    "print(p.stdout.decode())\n",
    "\n",
    "print('Total\\n')\n",
    "p = subprocess.run(['psql', '-d', 'finn', '-c', qry_af_tot], stdout=subprocess.PIPE, check=True)\n",
    "print(p.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all processing above is successful and you would like to reclaim the diskspace, you can change value of `wipe_intermediate_vector_in_db` defined in the first section of this noetbook to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans intermediate vector\n",
    "if wipe_intermediate_vector_in_db == True:\n",
    "    # wipe out completely\n",
    "    print(tag_af)\n",
    "    sch_af = 'af_%s' % tag_af\n",
    "    print(sch_af)\n",
    "    qry = 'DROP SCHEMA \"%s\" CASCADE;' % sch_af\n",
    "    cmd = ['psql',  '-d', os.environ[\"PGDATABASE\"], '-c', qry]\n",
    "    subprocess.run(cmd, check=True)\n",
    "elif clean_intermediate_vector_in_db == True:\n",
    "    # be slective and leave work_pnt (imported/cleaned points) and out_* (results being exported)\n",
    "    reload(run_extra)\n",
    "    run_extra.clean_db_af(tag_af, rasters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data for raster dataset are located in three different places.  First the original HDF format file you downloaded from EarthData website.  Seond is GeoTiff format data prepared for importing into PostGIS dataset.  The third is raster dataset inside PostGIS dataset.  Diskspace usage of each are summarized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ['du', '-csh', ddir_lct, ddir_vcf]\n",
    "\n",
    "p = subprocess.run(cmd, stdout=subprocess.PIPE)\n",
    "print('Disk use by downloaded raster hdf files')\n",
    "print(p.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = ['du', '-csh', workdir_lct, workdir_vcf]\n",
    "p = subprocess.run(cmd, stdout=subprocess.PIPE)\n",
    "print('Disk use by intermediate raster processing files')\n",
    "print(p.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qry_rst = \"\"\"SELECT table_schema || '.' || table_name AS table_full_name,\n",
    "pg_size_pretty(pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"')) AS size\n",
    "FROM information_schema.tables\n",
    "WHERE table_name ~ '^.*(%(tbl_lct)s|%(tbl_vcf)s)'   \n",
    "ORDER BY pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"') DESC;\"\"\" % dict(\n",
    "    tbl_lct=('rst_%s' % tag_lct),\n",
    "    tbl_vcf=('rst_%s' % tag_vcf),\n",
    ")\n",
    "qry_rst_tot = \"\"\"SELECT table_schema,\n",
    "pg_size_pretty(sum(pg_total_relation_size('\"' || table_schema || '\".\"' || table_name || '\"'))) AS size\n",
    "FROM information_schema.tables\n",
    "WHERE table_name ~ '^.*(%(tbl_lct)s|%(tbl_vcf)s)'   \n",
    "GROUP BY table_schema;\"\"\" % dict(\n",
    "    sch_af=('af_%s' % tag_af),\n",
    "    tbl_lct=('rst_%s' % tag_lct),\n",
    "    tbl_vcf=('rst_%s' % tag_vcf),\n",
    ")\n",
    "print('Disk use by raster dataset in the database\\n')\n",
    "p = subprocess.run(['psql', '-d', 'finn', '-c', qry_rst], stdout=subprocess.PIPE, check=True)\n",
    "print(p.stdout.decode())\n",
    "print('Total for %(tag_lct)s and %(tag_vcf)s\\n' % dict(tag_lct=tag_lct, tag_vcf=tag_vcf))\n",
    "p = subprocess.run(['psql', '-d', 'finn', '-c', qry_rst_tot], stdout=subprocess.PIPE, check=True)\n",
    "print(p.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans intermediate raster\n",
    "\n",
    "if wipe_downloaded_hdf == True:\n",
    "    # ditch entire download directory for the year\n",
    "    tgts = [ddir_lct, ddir_vcf]\n",
    "    cmd = ['rm', '-fr', ] + tgts\n",
    "    print(cmd)\n",
    "    subprocess.run(cmd, check=True)\n",
    "    \n",
    "if wipe_intermediate_geotiff == True:\n",
    "    # ditch entire processing directory \n",
    "    tgts = [workdir_lct, workdir_vcf]\n",
    "    cmd = ['rm', '-fr', ] + tgts\n",
    "    print(cmd)\n",
    "    subprocess.run(cmd, check=True)\n",
    "    \n",
    "\n",
    "if wipe_intermediate_rst_in_db == True:\n",
    "    # delete each table from the schema 'raster'\n",
    "    reload(rst_import)\n",
    "    \n",
    "    rst_import.drop_tables(tag_lct)\n",
    "    rst_import.drop_tables(tag_vcf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save hardcopy of this notebook as html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_html_of_notebook:\n",
    "    notebook_util.save_checkpoint()\n",
    "    nb_path = notebook_util.notebook_path()\n",
    "    print(nb_path)\n",
    "    time.sleep(5)\n",
    "    status = os.system('jupyter nbconvert --to html ' + os.path.basename(nb_path))\n",
    "    if status == 0:\n",
    "        print('html saved')\n",
    "    else:\n",
    "        print('save html failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
