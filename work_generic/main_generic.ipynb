{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# FINN Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. User specified configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You specify three items specific to analysis.\n",
    "\n",
    "1. `tag_af`: a string that identifies this active fire dataset through the processing.\n",
    "\n",
    "2. `af_fnames`: list of path for the shape files you downloaded from FIRMS website  \n",
    "\n",
    "Files you specified as `af_names` will be imported into a databse schema \"af_<i>tag_af</i>\", and processed in the database.  \n",
    "Final output file will be \"out_<i>tag_af</i>_*.csv\" and \"out_<i>tag_af</i>_*.shp\".\n",
    "\n",
    "3. `year_rst`: MODIS raster data year to be used for the analysis\n",
    "\n",
    "Other parameters  such as `tag_lct`, `tag_vcf`, `tag_regnum` (identify landcover, vcf and region number dataset) are set automatically for MODIS dataset.\n",
    "\n",
    "The datasets are imported into database schema \"raster\", with table names \"rst_<i>tag_lct</i>\", or \"rst_modlct_2017\", for example.  \n",
    "\n",
    "An overview raster \"o_32_rst_modlct_2007\" is created as well, as the real dataset is difficult to handle for QA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you have example for the first cell.\n",
    "\n",
    "Example 1.  Example usage with small local dataset.  Uses both MODIS and VIIRS active fire, which came with the source code, in \"sample_datasets\" directory.\n",
    "```python\n",
    "# tag to identify active fire dataset\n",
    "tag_af = 'testOTS_092018'\n",
    "\n",
    "# active fire shp file name and path\n",
    "af_fnames = [\n",
    "    '../sample_datasets/fire/testOTS_092018/fire_archive_M6_23960.shp',\n",
    "    '../sample_datasets/fire/testOTS_092018/fire_archive_V1_23961.shp',\n",
    "]\n",
    "\n",
    "# MODIS raster datasets' year\n",
    "year_rst = 2017\n",
    "```\n",
    "\n",
    "Example 2.  Another local fire example, using NRT dataset\n",
    "\n",
    "```python\n",
    "# tag to identify active fire dataset\n",
    "# USA (excl. ALK) 7 days fire downloaded 2019-01-13 \n",
    "tag_af = 'testUSA_012019'\n",
    "\n",
    "# shp file names\n",
    "af_fnames = [\n",
    "    '../sample_datasets/fire/testUSA_012019/MODIS_C6_USA_contiguous_and_Hawaii_7d.shp',\n",
    "    '../sample_datasets/fire/testUSA_012019/VNP14IMGTDL_NRT_USA_contiguous_and_Hawaii_7d.shp',\n",
    "]\n",
    "\n",
    "# MODIS raster datasets' year\n",
    "year_rst = 2017\n",
    "```\n",
    "\n",
    "\n",
    "Example 3.  Example usage for global 2016 run.  AF file is provided by us but will be downloaded from AWS that we host.\n",
    "```python\n",
    "# tag to identify active fire dataset\n",
    "tag_af = 'mod_global_2016'\n",
    "\n",
    "# shp file names\n",
    "af_fnames = [\n",
    "    '../sample_datasets/fire/global_2016/fire_archive_M6_28864.shp',\n",
    "]\n",
    "\n",
    "# MODIS raster datasets' year\n",
    "year_rst = 2016\n",
    "```\n",
    "\n",
    "\n",
    "Example 4.\n",
    "If you have downloaded shape file for AF, here is an example.  AF file should reside somewhere under finn_preproc, i.e., where you git clone (or downloaded) the source.  For example, 'sample_datasets' directory came with the source code and can be accessed with the small local dataset example (ex 1.)\n",
    "```python\n",
    "# tag to identify active fire dataset\n",
    "tag_af = 'XXXXX'\n",
    "\n",
    "# active fire shp file name and path\n",
    "af_fnames = [\n",
    "]\n",
    "\n",
    "# MODIS raster datasets' year\n",
    "year_rst = XXX\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag to identify active fire dataset\n",
    "tag_af = 'mod_global_2016'\n",
    "\n",
    "# shp file names\n",
    "af_fnames = [\n",
    "    '../sample_datasets/fire/global_2016/fire_archive_M6_28864.shp',\n",
    "]\n",
    "\n",
    "# MODIS raster datasets' year\n",
    "year_rst = 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of code should run without user editing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Envoronments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import subprocess\n",
    "import shlex\n",
    "from urllib.parse import urlparse\n",
    "from importlib import reload\n",
    "import gdal\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "# finn preproc codes\n",
    "sys.path = sys.path + ['../code_anaconda']\n",
    "import downloader\n",
    "import af_import\n",
    "import rst_import\n",
    "import polygon_import\n",
    "import run_step1\n",
    "import run_step2\n",
    "import export_shp\n",
    "import plotter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import AF dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test active fire data files exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular sample AF dataset are provided by FINN developper.  In other applications, it will be user's resoponsibility to provide shape file for active fire in specified path/name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check input file exists\n",
    "print('checking if input files exist:')\n",
    "re_shp = re.compile('fire_archive_(.*).shp')\n",
    "re_zip = re.compile('DL_FIRE_(.*).shp')\n",
    "\n",
    "re_shp_nrt = re.compile('(MODIS_C6|VNP14IMGTDL_NRT)_(.*).shp')\n",
    "\n",
    "\n",
    "\n",
    "for i,af_fname in enumerate(af_fnames):\n",
    "    print(\"%s: \" % af_fname, end='')\n",
    "    \n",
    "    pn,fn = os.path.split(af_fname)\n",
    "    zname = None\n",
    "    \n",
    "    if os.path.exists(af_fname):\n",
    "        print(\"exists.\")\n",
    "        # if .zip file, need to expand.\n",
    "        if af_fname[-4:] == '.shp':\n",
    "            # you are good\n",
    "            print('OK')\n",
    "        \n",
    "        elif af_fname[-4:] == '.zip':\n",
    "            # still need to unzip\n",
    "            zname = af_fname\n",
    "            m = re_zip.match(af_fname)\n",
    "            if m:\n",
    "                arcname = m.group()[0]\n",
    "                sname = 'fire_archive_%s.shp' % arcname\n",
    "            else:\n",
    "                # i cannot predict name of shp file...\n",
    "                import zipfile\n",
    "                # find what shp file included...?\n",
    "                raise RuntileError('specify .shp file in af_names list!')\n",
    "                arcname,sname = None, None\n",
    "        else:\n",
    "            raise RuntimeError('specify .shp file in af_names list!')\n",
    "    else:\n",
    "        print(\"doesn't exist.\")\n",
    "        \n",
    "        if af_fname[-4:] == '.shp':\n",
    "            # guess the zip file name\n",
    "            \n",
    "            pn,fn=os.path.split(af_fname)\n",
    "            \n",
    "            # see if it's the sample giant archive we provide \n",
    "            if fn == 'fire_archive_M6_28864.shp':\n",
    "                zurl = 'https://s3-us-west-2.amazonaws.com/earthlab-finn/2016-global-DL_FIRE_M6_28864.zip'\n",
    "                zn = '2016-global-DL_FIRE_M6_28864.zip'\n",
    "                zname = os.path.join(pn, zn)\n",
    "                sname = fn\n",
    "                if not os.path.exists(zname):\n",
    "                    print('downloading the sample AF file: %s' % zn)\n",
    "                    subprocess.run(['wget', '-P', pn, zurl], check=True)\n",
    "            else:\n",
    "\n",
    "                # see if it's an archive of AF\n",
    "                m = re_shp.match(fn)\n",
    "                if m:\n",
    "                    archname = m.groups()[0]\n",
    "                    zname = os.path.join( pn, 'DL_FIRE_%s.zip' % arcname)\n",
    "                    sname = fn\n",
    "                    print('  found zip: %s' % zname)\n",
    "                else:\n",
    "                    # see if it's NRT data\n",
    "                    m = re_shp_nrt.match(fn)\n",
    "\n",
    "                    if m:\n",
    "                        # NRT downloads\n",
    "                        zname = af_fname[:-4] + '.zip'\n",
    "                        sname = fn\n",
    "                        print('  found zip: %s' % zname)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        raise RuntimeError('cannot find file: %s' % af_fname)\n",
    "        else:\n",
    "            raise RuntimeError('cannot find file: %s' % af_fname)\n",
    "    if zname:\n",
    "        print('unzipping: %s' % zname)\n",
    "        subprocess.run(['unzip', '-uo', zname, '-d', os.path.dirname(zname)],\n",
    "                      check=True)\n",
    "        assert os.path.exists(os.path.join(pn, sname))\n",
    "        af_fnames[i] = os.path.join(pn, sname)\n",
    "        print('OK: done')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import active fire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and import into database.\n",
    "\n",
    "<b>Be careful!!</b> The code has no safe guard and wipe the schema for the scheama \"af_<i>tag_af</i>\" and starts over.  \n",
    "\n",
    "Let me think the design a bit more for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(af_import)\n",
    "\n",
    "# TODO this is destructive need to safe guard!\n",
    "# tell user schema is there, list table names and # of row of each.  Ask her to delete manually or something to proceed\n",
    "af_import.main(tag_af, af_fnames)\n",
    "\n",
    "print()\n",
    "for i,fn in enumerate(af_fnames):\n",
    "    print(fn)\n",
    "    p = subprocess.run(['psql', '-c', 'select count(*) from \"af_%s\".af_in_%d;' % (tag_af, i+1)], stdout=subprocess.PIPE)\n",
    "    print(p.stdout.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download raster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings for Land Surface Datasets (land cover, vegetation continuous field, region definieons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag to identify datasets, automatically set to be modlct_YYYY, modvcf_YYYY\n",
    "tag_lct = 'modlct_%d' % year_rst\n",
    "tag_vcf = 'modvcf_%d' % year_rst\n",
    "\n",
    "# tag for the region number polygon\n",
    "tag_regnum = 'regnum'\n",
    "\n",
    "# definition of variables in the raster files\n",
    "rasters = [\n",
    "        {\n",
    "            'tag': tag_lct,\n",
    "            'kind': 'thematic',\n",
    "            'variable': 'lct'\n",
    "        },\n",
    "        {\n",
    "            'tag': tag_vcf,\n",
    "            'kind': 'continuous',\n",
    "            'variables': ['tree', 'herb', 'bare'],\n",
    "        },\n",
    "        {\n",
    "            'tag': tag_regnum,\n",
    "            'kind': 'polygons',\n",
    "            'variable_in': 'region_num',\n",
    "            'variable': 'regnum',\n",
    "        },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the extent of raster dataset in the database encloses all fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that raster data covers extent of AF data\n",
    "reload(af_import)\n",
    "dct = {}\n",
    "for i,fn in enumerate(af_fnames):\n",
    "    for tag_rst in (tag_lct, tag_vcf):\n",
    "        if len(af_fnames) == 1:\n",
    "            cnts = af_import.check_raster_contains_fire(\n",
    "                '\"raster\".\"skel_rst_%s\"' % tag_rst, \n",
    "                '\"af_%s\".\"af_in\"' % (tag_af)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            cnts = af_import.check_raster_contains_fire(\n",
    "                '\"raster\".\"skel_rst_%s\"' % tag_rst, \n",
    "                '\"af_%s\".\"af_in_%d\"' % (tag_af, i+1)\n",
    "            )\n",
    "        print(os.path.basename(fn), tag_rst, cnts)\n",
    "        dct[(fn,tag_rst)] = cnts\n",
    "\n",
    "        \n",
    "# **TODO** In some case \"fire\" is ditected over the the ocean and there is no raster for that part of earth\n",
    "# need to check if that's the case for the 'n_not_contained' fire\n",
    "\n",
    "need_to_import_raster = False\n",
    "if any(_['n_not_contained'] > 0 for _ in  dct.values()):\n",
    "    print('Some fire is not conained in raster')\n",
    "    print('Will download/import raster dataset')\n",
    "    need_to_import_raster = True\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raster files URL and directories to save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    # all raster downloads are stored in following dir\n",
    "    download_rootdir = '../downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    # earthdata's URL for landcover and VCF\n",
    "    is_leap = (year_rst % 4 == 0)\n",
    "    url_lct = 'https://e4ftl01.cr.usgs.gov/MOTA/MCD12Q1.006/%d.01.01/' % year_rst\n",
    "    url_vcf = 'https://e4ftl01.cr.usgs.gov/MOLT/MOD44B.006/%d.03.%02d/' % (year_rst, 5 if is_leap else 6)\n",
    "\n",
    "    ddir_lct = download_rootdir +'/'+ ''.join(urlparse(url_lct)[1:3])\n",
    "    ddir_vcf = download_rootdir +'/'+ ''.join(urlparse(url_vcf)[1:3])\n",
    "\n",
    "    print('LCT downloads goes to %s' % ddir_lct)\n",
    "    print('VCF downloads goes to %s' % ddir_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download land cover type raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    reload(downloader)\n",
    "    downloader.download_only_needed(url = url_lct, droot = download_rootdir, pnts=af_fnames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify LCT files' checksum.  If a file is correpted, the file is downloaded again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    downloader.purge_corrupted(ddir = ddir_lct, url=url_lct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do similar for vegetation continuous field data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    downloader.download_only_needed(url = url_vcf, droot = download_rootdir, pnts=af_fnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    downloader.purge_corrupted(ddir_vcf, url=url_vcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Import raster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded files need preprocessing, which is to extract the only raster band needed, and also make coordinate system to be WGS84.  Intermediate files are created in following directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir_lct = '../proc_rst_%s' % tag_lct\n",
    "workdir_vcf = '../proc_rst_%s' % tag_vcf\n",
    "workdir_regnum = '../proc_rst_%s' % tag_regnum\n",
    "\n",
    "print('LCT preprocessing occurs in %s' % workdir_lct)\n",
    "print('VCF preprocessing occurs in %s' % workdir_vcf)\n",
    "print('RegNum preprocessing occurs in %s' % workdir_regnum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import land cover type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First grab hdf file names from the download directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    search_string = \"%(ddir_lct)s/MCD12Q1.A%(year_rst)s001.h??v??.006.*.hdf\" % dict(\n",
    "        ddir_lct = ddir_lct, year_rst=year_rst)\n",
    "    fnames_lct = sorted(glob.glob(search_string))\n",
    "    print('found %d hdf files' % len(fnames_lct) )\n",
    "    if len(fnames_lct) == 0:\n",
    "        raise RuntimeError(\"check if downloads are successful and search string to be correct: %s\" % search_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command performs three tasks, \"merge\", \"resample\" and \"import\".  First two task creates intermediate GeoTiff files in <i>work_dir</i>.  Last task actually import the data into database's <i>raster</i> schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    reload(rst_import)\n",
    "\n",
    "    rst_import.main(tag_lct, fnames=fnames_lct, workdir = workdir_lct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should able to see the raster in the database using QGIS.  \n",
    "I am also trying to make quick check here creating simple image for QA, but use of GIS tool is encouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import plotter\n",
    "reload(plotter)\n",
    "try:\n",
    "    plotter.plot('raster.o_32_rst_%s' % tag_lct, '../code_anaconda/modlct.clr')\n",
    "except Exception as e:\n",
    "    print(\"Got this error: \" + str(e))\n",
    "    print(\"Didn't work, use QGIS!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import vegetation continuous fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous steps repeated for vegetation continous fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    # grab hdf file names\n",
    "    search_string = \"%(ddir_vcf)s/MOD44B.A%(year)s065.h??v??.006.*.hdf\" % dict(\n",
    "            ddir_vcf = ddir_vcf, year=year_rst)\n",
    "    fnames_vcf = sorted(glob.glob(search_string))\n",
    "    print('found %d hdf files' % len(fnames_vcf) )\n",
    "    if len(fnames_vcf) == 0:\n",
    "        raise RuntimeError(\"check if downloads are successfull and search string to be correct: %s\" % search_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if need_to_import_raster:\n",
    "    reload(rst_import)\n",
    "    rst_import.main(tag_vcf, fnames=fnames_vcf, workdir = workdir_vcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import plotter\n",
    "reload(plotter)\n",
    "try:\n",
    "    plotter.plot('raster.o_32_rst_%s' % tag_vcf)\n",
    "except Exception as e:\n",
    "    print(\"Got this error: \" + str(e))\n",
    "    print(\"Didn't work, use QGIS!\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import countries of the world shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually not a raster but vector data of polygons.  But since it serves conceptually similar function as raster (specify attribute for a given geographic location), I treat it as if it is a raster dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(workdir_regnum, 'All_Countries.shp')):\n",
    "    subprocess.run(['wget', '-P', workdir_regnum, \n",
    "                    'https://s3-us-west-2.amazonaws.com/earthlab-finn/All_Countries.zip'], \n",
    "                   check=True)\n",
    "    subprocess.run(['unzip', os.path.join(workdir_regnum, 'All_Countries.zip'), '-d' , workdir_regnum ], \n",
    "                  check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(polygon_import)\n",
    "polygon_import.main('regnum', shpname = os.path.join(workdir_regnum, 'All_Countries.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process active fire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running \"step 1\" grouping points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload(run_step1)\n",
    "\n",
    "if 'firstday' not in dir(): firstday = None\n",
    "if 'lastday' not in dir(): lastday = None\n",
    "run_step1.main(tag_af, firstday=firstday, lastday=lastday, ver='v7m', run_prep = True, run_work=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running \"step 2\" intersection with raster datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(run_step2)\n",
    "\n",
    "assert run_step2.ver == 'v8b'\n",
    "run_step2.main(tag_af, rasters, firstday=firstday, lastday=lastday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default output directory is this diretory (where you have this Jupyter Notebook file), and output file has long name of having tag of each datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '.'\n",
    "shpname = 'out_{0}_{1}_{2}_{3}.shp'.format(tag_af, tag_lct, tag_vcf, tag_regnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'af_' + tag_af\n",
    "tblname = 'out_{0}_{1}_{2}'.format(tag_lct, tag_vcf, tag_regnum)\n",
    "flds = ('v_lct', 'f_lct', 'v_tree', 'v_herb', 'v_bare', 'v_regnum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[TODO]** tell where these file went in the file sytem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(export_shp)\n",
    "export_shp.main(outdir, schema, tblname, flds, shpname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
